# -*- coding: utf-8 -*-
"""earthquake_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WgZSERN9UxzP_BS-UkGJVdTM8vq9libn
"""

from google.colab import auth
auth.authenticate_user()

!pip install --upgrade google-cloud-bigquery

from google.cloud import bigquery

client = bigquery.Client(project='earthquake_project')

import requests
import pandas as pd
from google.cloud import bigquery

# Pull JSON data from USGS
url = "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson"
response = requests.get(url)
data = response.json()

# Flatten JSON
earthquakes = pd.json_normalize(data['features'])

# Keep important columns
earthquakes = earthquakes[['properties.time','properties.place','properties.mag','geometry.coordinates']]

# Split coordinates
earthquakes[['longitude','latitude','depth']] = pd.DataFrame(
    earthquakes['geometry.coordinates'].tolist(), index=earthquakes.index
)

# Drop original column & rename
earthquakes.drop(columns=['geometry.coordinates'], inplace=True)
earthquakes.rename(columns={
    'properties.time': 'time',
    'properties.place': 'place',
    'properties.mag': 'magnitude'
}, inplace=True)

# Convert timestamp to datetime
earthquakes['time'] = pd.to_datetime(earthquakes['time'], unit='ms')

print(earthquakes.head())

# Initialize BigQuery client
client = bigquery.Client(project='earthquake_project')
dataset_id = 'earthquake_dataset'
table_id = 'earthquakes'

!pip install --upgrade pandas-gbq
from pandas_gbq import to_gbq

dataset_id = 'earthquake_dataset'
table_id = 'earthquakes'
table_ref = f"{dataset_id}.{table_id}"
project_id = 'earthquake-project-469810'

to_gbq(
    dataframe=earthquakes,
    destination_table= f"{dataset_id}.{table_id}",
    project_id= 'earthquake-project-469810',
    if_exists='append'
)
print("Upload complete!")